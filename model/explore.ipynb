{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import os\n",
    "from scripts import text_utils\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_text = text_utils.get_two_cities_sample(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = text_utils.clean_up_text(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the period \\n it was the best of times , it was the worst of times , it was the age of wisdom , it was the age of foolishness , it was the epoch of belief , it was the epoch of incredulity , it was the'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert text corpus to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'period', '\\n', 'it', 'was', 'the', 'best', 'of', 'times', ',']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split(' ')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "0\n",
      "spending\n",
      "[1, 2, 3, 4, 5, 1, 6, 7, 8, 9, 4, 5, 1, 10, 7, 8, 9, 4, 5, 1, 11, 7, 12, 9, 4, 5, 1, 11, 7, 13, 9, 4, 5, 1, 14, 7, 15, 9, 4, 5, 1, 14, 7, 16, 9, 4, 5, 1, 17, 7, 18, 9, 4, 5, 1, 17, 7, 19, 9, 4, 5, 1, 20, 7, 21, 9, 4, 5, 1, 22, 7, 23, 9, 24, 25, 26, 27, 28, 9, 24, 25, 29, 27, 28, 9, 24, 30, 31, 32, 33, 34, 35, 9, 24, 30, 31, 32, 33, 1, 36]\n",
      "['period', '\\n', 'was', 'best', 'it', 'period', 'best']\n"
     ]
    }
   ],
   "source": [
    "word_dict = text_utils.WordDict(text.split(' '))\n",
    "print(word_dict.get_id('light'))\n",
    "print(word_dict.get_id('the2'))\n",
    "print(word_dict.get_token(200))\n",
    "print(word_dict.tokens_to_ids(text.split(' ')[:100]))\n",
    "print(word_dict.ids_to_tokens([2,3,5,6,4,2,6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5557\n"
     ]
    }
   ],
   "source": [
    "print(word_dict.get_vocab_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size = word_dict.get_vocab_size() # TODO\n",
    "embedding_size = 300\n",
    "batch_size = 64\n",
    "paragraph_length = 30\n",
    "hidden_dim = 40\n",
    "\n",
    "lstm_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # hyperparameters\n",
    "# num_sampled = 3\n",
    "\n",
    "# # build the graph\n",
    "# g = tf.Graph()\n",
    "# with g.as_default():\n",
    "#   embeddings = tf.Variable(\n",
    "#     tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0),\n",
    "#     name=\"embeddings\")\n",
    "#   words = tf.placeholder(tf.int32, shape=[batch_size, paragraph_length])\n",
    "  \n",
    "#   # initial hidden state\n",
    "#   h0 = tf.placeholder(tf.float32, shape=[hidden_dim])\n",
    "#   # inputs to cells\n",
    "#   X = tf.nn.embedding_lookup(embeddings, words)\n",
    "\n",
    "#   cell = tf.contrib.rnn.BasicLSTMCell(num_cells)\n",
    "  \n",
    "  \n",
    "#   cell(hp)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "  cell = tf.contrib.rnn.LSTMCell(200)\n",
    "  print(cell.state_size.h)\n",
    "  # Initial state of the LSTM memory.\n",
    "#   state = tf.zeros([batch_size, lstm.state_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53740,)\n",
      "(53740,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# prepare feed data for training\n",
    "data_full = np.array(word_dict.tokens_to_ids(text.split(' ')))\n",
    "data_input = data_full[0:-1]\n",
    "data_label = data_full[1:]\n",
    "\n",
    "print(data_input.shape)\n",
    "print(data_label.shape)\n",
    "\n",
    "def gen_batch(batch_size):\n",
    "  p = np.random.choice(range(len(data_input)), size=batch_size)\n",
    "  return {\n",
    "    train_inputs: data_input[p],\n",
    "    train_labels: data_label[p, np.newaxis]\n",
    "  }\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LOG_DIR = './visual/embed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write vocab to metadata.tsv\n",
    "with open(os.path.join(LOG_DIR, 'metadata.tsv'), 'w') as f:\n",
    "  for i in range(word_dict.get_vocab_size()):\n",
    "    if word_dict.get_token(i) == '\\n':\n",
    "      f.write('\\\\n' + '\\n')\n",
    "    else:\n",
    "      f.write(word_dict.get_token(i) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "config = projector.ProjectorConfig()\n",
    "embedding_config = config.embeddings.add()\n",
    "embedding_config.tensor_name = embeddings.name\n",
    "embedding_config.metadata_path = os.path.join(LOG_DIR, 'metadata.tsv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0, training loss: 19.49799919128418\n",
      "iter: 100, training loss: 12.179189682006836\n",
      "iter: 200, training loss: 18.494905471801758\n",
      "iter: 300, training loss: 19.68937110900879\n",
      "iter: 400, training loss: 22.087669372558594\n",
      "iter: 500, training loss: 19.459903717041016\n",
      "iter: 600, training loss: 11.850862503051758\n",
      "iter: 700, training loss: 16.97174835205078\n",
      "iter: 800, training loss: 17.621646881103516\n",
      "iter: 900, training loss: 19.054561614990234\n",
      "iter: 1000, training loss: 14.592487335205078\n",
      "iter: 1100, training loss: 14.503328323364258\n",
      "iter: 1200, training loss: 18.040023803710938\n",
      "iter: 1300, training loss: 11.424406051635742\n",
      "iter: 1400, training loss: 12.496201515197754\n",
      "iter: 1500, training loss: 5.899695873260498\n",
      "iter: 1600, training loss: 14.117030143737793\n",
      "iter: 1700, training loss: 14.185291290283203\n",
      "iter: 1800, training loss: 9.79128360748291\n",
      "iter: 1900, training loss: 11.126241683959961\n",
      "iter: 2000, training loss: 17.155893325805664\n",
      "iter: 2100, training loss: 11.354045867919922\n",
      "iter: 2200, training loss: 17.101768493652344\n",
      "iter: 2300, training loss: 3.3084821701049805\n",
      "iter: 2400, training loss: 9.405471801757812\n",
      "iter: 2500, training loss: 13.50086784362793\n",
      "iter: 2600, training loss: 2.014860153198242\n",
      "iter: 2700, training loss: 12.449623107910156\n",
      "iter: 2800, training loss: 18.56908416748047\n",
      "iter: 2900, training loss: 10.28596305847168\n",
      "iter: 3000, training loss: 16.228633880615234\n",
      "iter: 3100, training loss: 9.252752304077148\n",
      "iter: 3200, training loss: 7.133919715881348\n",
      "iter: 3300, training loss: 3.1666831970214844\n",
      "iter: 3400, training loss: 9.402042388916016\n",
      "iter: 3500, training loss: 10.613380432128906\n",
      "iter: 3600, training loss: 6.722871780395508\n",
      "iter: 3700, training loss: 8.674348831176758\n",
      "iter: 3800, training loss: 1.9484037160873413\n",
      "iter: 3900, training loss: 2.8419878482818604\n",
      "iter: 4000, training loss: 5.93376350402832\n",
      "iter: 4100, training loss: 2.2877085208892822\n",
      "iter: 4200, training loss: 4.409359455108643\n",
      "iter: 4300, training loss: 1.9769871234893799\n",
      "iter: 4400, training loss: 1.8444045782089233\n",
      "iter: 4500, training loss: 10.27267074584961\n",
      "iter: 4600, training loss: 19.19553565979004\n",
      "iter: 4700, training loss: 22.184934616088867\n",
      "iter: 4800, training loss: 13.480938911437988\n",
      "iter: 4900, training loss: 12.687959671020508\n",
      "iter: 5000, training loss: 15.32039737701416\n",
      "iter: 5100, training loss: 2.8040926456451416\n",
      "iter: 5200, training loss: 5.832302093505859\n",
      "iter: 5300, training loss: 15.830733299255371\n",
      "iter: 5400, training loss: 18.27777671813965\n",
      "iter: 5500, training loss: 1.3531134128570557\n",
      "iter: 5600, training loss: 10.607905387878418\n",
      "iter: 5700, training loss: 9.632471084594727\n",
      "iter: 5800, training loss: 1.2545077800750732\n",
      "iter: 5900, training loss: 10.631729125976562\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=g) as sess:\n",
    "  graph_writer = tf.summary.FileWriter(LOG_DIR)\n",
    "  graph_writer.add_graph(sess.graph)\n",
    "  \n",
    "  projector.visualize_embeddings(graph_writer, config)\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  \n",
    "  saver = tf.train.Saver()\n",
    "  \n",
    "  batch_size = 32\n",
    "  for i in range(6000):\n",
    "    _, cur_loss = sess.run([train_op, loss], feed_dict=gen_batch(batch_size))\n",
    "    if i % 100 == 0:\n",
    "      print(\"iter: {}, training loss: {}\".format(i, cur_loss))\n",
    "    \n",
    "  saver.save(sess, os.path.join(LOG_DIR, 'embed_model.ckpt'), global_step=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
