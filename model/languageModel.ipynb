{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import os\n",
    "from scripts import text_utils\n",
    "from scripts import language_model\n",
    "from scripts import embed_meta\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_data(num_lines=None):\n",
    "  books_data = text_utils.get_books_data()\n",
    "  if num_lines != None:\n",
    "    books_data = books_data[:num_lines]\n",
    "  text = text_utils.clean_up_text(books_data).split(' ')\n",
    "  word_dict = text_utils.WordDict(text)\n",
    "  text_data = word_dict.tokens_to_ids(text)\n",
    "  vocab_size = word_dict.get_vocab_size()\n",
    "  print(\"vocab size is {}\".format(vocab_size))\n",
    "  print(\"total number of words is {}\".format(text_data.shape[0]))\n",
    "  return {\n",
    "    'word_dict': word_dict,\n",
    "    'text_data': text_data\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_batch(text_data, batch_size, T):\n",
    "  data = np.zeros([batch_size, T+1], dtype=np.int64)\n",
    "  starts = np.random.choice(range(0, text_data.shape[0]-T-1), size=batch_size)\n",
    "  for i in range(len(starts)):\n",
    "    data[i, :] = text_data[starts[i]:starts[i]+T+1]\n",
    "  input_data = data[:, 0:-1]\n",
    "  labels = data[:, 1:]\n",
    "  return input_data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of lines: 258521\n",
      "vocab size is 8000\n",
      "total number of words is 2715459\n"
     ]
    }
   ],
   "source": [
    "data = get_data()\n",
    "word_dict = data['word_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000000,)\n"
     ]
    }
   ],
   "source": [
    "num_train = 2000000\n",
    "num_val   =  400000\n",
    "text_data_trn = data['text_data'][:num_train]\n",
    "text_data_val = data['text_data'][num_train:num_train+num_val]\n",
    "# text_data_val = data['text_data']\n",
    "print(text_data_trn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "LOG_DIR = './results/'\n",
    "model_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "T = 40 # number of words in one sample\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "  global_step = tf.Variable(0)\n",
    "  with tf.variable_scope(\"Model\", initializer=tf.random_uniform_initializer(-0.04, 0.04)):\n",
    "    model = language_model.LanguageModel(batch_size, T, global_step, word_dict.get_vocab_size())\n",
    "  tf.summary.scalar(\"loss\", model.loss)\n",
    "  merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['short', 'as', 'never', 'dragon', '<unk>', '<unk>', 'or', 'other', 'animal', 'of', 'that', 'species', 'presided', 'over', 'since', 'they', 'first', 'began', 'to', 'interest', 'themselves', 'in', 'household', 'affairs', '.', '\\n', 'an', 'old', 'gentleman', 'and']\n"
     ]
    }
   ],
   "source": [
    "batch_data, _ = get_batch(text_data_trn, 2, T)\n",
    "print(word_dict.ids_to_tokens(batch_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0, loss_trn: 8.9872, perplexity_trn: 7999.99\n",
      "iter: 0, loss_val: 8.9744, perplexity_val: 7898.42\n",
      "['in', 'a', 'warm', 'corner', 'of', 'which', 'he', 'stretched', 'his', 'weary', 'limbs', 'and', 'soon', 'fell', 'asleep', '.', '\\n', 'when', 'he', 'awoke', 'next', 'morning', 'and', 'tried', 'to', 'recollect', 'his', 'dreams', 'which', 'got', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that']\n",
      "['the', 'glass', '.', 'and', 'thus', 'he', 'knew', 'and', 'by', 'returning', 'it', 'gave', 'mr', 'pecksniff', 'the', 'information', 'that', 'he', 'knew', 'where', 'the', 'listener', 'had', 'been', ';', 'and', 'that', 'instead', 'of', 'hat', 'the', 'the', 'the', 'the', 'the', 'the', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that']\n",
      "['rear', 'who', 'were', 'pressing', 'forward', 'to', 'get', 'out', 'of', 'the', 'way', 'but', 'were', 'detained', 'for', 'a', 'few', 'moments', 'by', 'the', 'throng', 'in', 'front', 'succeeded', 'in', 'clearing', 'the', 'room', 'when', 'believe', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that']\n",
      "['miss', 'squeers', 'looked', 'sideways', 'at', 'her', 'father', 'again', 'who', 'looked', 'sideways', 'at', 'her', 'as', 'much', 'as', 'to', 'say', '‘there', 'you', 'had', 'him', '.', '’', '\\n', '‘i', 'know', 'what', 'you’ve', 'daughter', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that']\n",
      "['when', 'they', 'reached', 'the', 'bow', '-', '<unk>', 'front', 'room', 'on', 'the', 'first', 'floor', '.', '\\n', 'nicholas', 'bowed', 'his', '<unk>', 'and', 'was', '<unk>', 'glad', 'to', 'see', 'the', 'cloth', 'laid', '.', 'as', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that']\n",
      "iter: 10, loss_trn: 6.7481, perplexity_trn: 852.44\n",
      "iter: 10, loss_val: 6.8698, perplexity_val: 962.73\n",
      "iter: 20, loss_trn: 6.4393, perplexity_trn: 625.97\n",
      "iter: 20, loss_val: 6.4978, perplexity_val: 663.66\n",
      "iter: 30, loss_trn: 6.3542, perplexity_trn: 574.91\n",
      "iter: 30, loss_val: 6.3952, perplexity_val: 598.94\n",
      "iter: 40, loss_trn: 6.2707, perplexity_trn: 528.85\n",
      "iter: 40, loss_val: 6.4679, perplexity_val: 644.11\n",
      "iter: 50, loss_trn: 6.3562, perplexity_trn: 576.03\n",
      "iter: 50, loss_val: 6.4875, perplexity_val: 656.91\n",
      "iter: 60, loss_trn: 6.1330, perplexity_trn: 460.83\n",
      "iter: 60, loss_val: 6.4360, perplexity_val: 623.93\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-d7d6c4dac968>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_training\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     }\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches_trn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mtrn_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'summary'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/keping/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/keping/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/keping/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/keping/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/keping/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=g) as sess:\n",
    "  writer = tf.summary.FileWriter(os.path.join(LOG_DIR), sess.graph)\n",
    "  fetches_trn = {\n",
    "    \"loss\": model.loss,\n",
    "    \"perplexity\": model.perplexity,\n",
    "    \"train_op\": model.train_op,\n",
    "    \"summary\": merged\n",
    "  }\n",
    "  fetches_val = {\n",
    "    \"loss\": model.loss,\n",
    "    \"perplexity\": model.perplexity,\n",
    "    \"summary\": merged\n",
    "  }\n",
    "  model_count += 1\n",
    "  trn_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, \"trn\"+str(model_count)))\n",
    "  val_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, \"val\"+str(model_count)))\n",
    "  \n",
    "  sess.run(tf.global_variables_initializer())\n",
    "\n",
    "  for i in range(20000):\n",
    "    input_data, labels = get_batch(text_data_trn, batch_size, T)\n",
    "    feed_dict = {\n",
    "      model.input_data: input_data,\n",
    "      model.labels: labels,\n",
    "      model.is_training: True\n",
    "    }\n",
    "    result = sess.run(fetches_trn, feed_dict)\n",
    "    trn_writer.add_summary(result['summary'], i)\n",
    "    if i % 10 == 0:\n",
    "      print(\"iter: {}, loss_trn: {:.4f}, perplexity_trn: {:.2f}\".format(\n",
    "          i, result['loss'], result['perplexity']))\n",
    "      input_data, labels = get_batch(text_data_val, batch_size, T)\n",
    "      feed_dict = {\n",
    "        model.input_data: input_data,\n",
    "        model.labels: labels,\n",
    "        model.is_training: False\n",
    "      }\n",
    "      result = sess.run(fetches_val, feed_dict)\n",
    "      print(\"iter: {}, loss_val: {:.4f}, perplexity_val: {:.2f}\".format(\n",
    "          i, result['loss'], result['perplexity']))\n",
    "      val_writer.add_summary(result['summary'], i)\n",
    "    if i % 200 == 0: # show predictions\n",
    "      predictions = sess.run(model.predictions, feed_dict)\n",
    "      for data_id, prediction in enumerate(predictions[:5]):\n",
    "        print(word_dict.ids_to_tokens(np.concatenate(\n",
    "            [feed_dict[model.input_data][data_id][:-1], \n",
    "             np.array(prediction)])))\n",
    "    if i != 0 and i % 5000 == 0:\n",
    "      saver = tf.train.Saver()\n",
    "      saver.save(sess, os.path.join(LOG_DIR, 'language_model{}.ckpt'.format(model_count)), global_step=i)\n",
    "      embed_meta.write_embed_meta(LOG_DIR, word_dict, model.embeddings, writer)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
