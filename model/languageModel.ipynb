{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import os\n",
    "from scripts import text_utils\n",
    "from scripts import language_model\n",
    "from scripts import embed_meta\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_data(num_lines):\n",
    "  books_data = text_utils.get_books_data()[:num_lines]\n",
    "  text = text_utils.clean_up_text(books_data).split(' ')\n",
    "  word_dict = text_utils.WordDict(text)\n",
    "  text_data = word_dict.tokens_to_ids(text)\n",
    "  vocab_size = word_dict.get_vocab_size()\n",
    "  print(\"vocab size is {}\".format(vocab_size))\n",
    "  print(\"total number of words is {}\".format(text_data.shape[0]))\n",
    "  return {\n",
    "    'word_dict': word_dict,\n",
    "    'text_data': text_data\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(text_data, batch_size, T):\n",
    "  data = np.zeros([batch_size, T+1], dtype=np.int64)\n",
    "  starts = np.random.choice(range(0, text_data.shape[0]-T-1), size=batch_size)\n",
    "  for i in range(len(starts)):\n",
    "    data[i, :] = text_data[starts[i]:starts[i]+T+1]\n",
    "  input_data = data[:, 0:-1]\n",
    "  labels = data[:, 1:]\n",
    "  return input_data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of lines: 258521\n",
      "vocab size is 7632\n",
      "total number of words is 132578\n"
     ]
    }
   ],
   "source": [
    "num_lines = 10000\n",
    "data = get_data(num_lines)\n",
    "word_dict = data['word_dict']\n",
    "text_data_trn = data['text_data'][:80000]\n",
    "text_data_val = data['text_data'][80000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "T = 20 # number of words in one sample\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "  global_step = tf.Variable(0)\n",
    "  model = language_model.LanguageModel(batch_size, T, global_step, word_dict.get_vocab_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LOG_DIR = './results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0, loss: 8.9401, perplexity: 7632.07\n",
      "iter: 5, loss: 8.9318, perplexity: 7569.23\n",
      "iter: 10, loss: 8.9104, perplexity: 7408.87\n",
      "iter: 15, loss: 8.7289, perplexity: 6178.66\n",
      "iter: 20, loss: 7.7738, perplexity: 2377.53\n",
      "iter: 25, loss: 6.9199, perplexity: 1012.25\n",
      "iter: 30, loss: 6.5554, perplexity: 703.02\n",
      "iter: 35, loss: 6.4610, perplexity: 639.70\n",
      "iter: 40, loss: 6.3000, perplexity: 544.57\n",
      "iter: 45, loss: 6.0863, perplexity: 439.78\n",
      "iter: 50, loss: 6.3244, perplexity: 558.00\n",
      "iter: 55, loss: 6.1888, perplexity: 487.28\n",
      "iter: 60, loss: 6.1271, perplexity: 458.12\n",
      "iter: 65, loss: 6.2029, perplexity: 494.18\n",
      "iter: 70, loss: 6.2022, perplexity: 493.85\n",
      "iter: 75, loss: 6.1959, perplexity: 490.73\n",
      "iter: 80, loss: 6.0840, perplexity: 438.79\n",
      "iter: 85, loss: 6.0986, perplexity: 445.22\n",
      "iter: 90, loss: 6.2179, perplexity: 501.64\n",
      "iter: 95, loss: 6.1012, perplexity: 446.41\n",
      "iter: 100, loss: 6.1575, perplexity: 472.24\n",
      "iter: 105, loss: 6.0532, perplexity: 425.46\n",
      "iter: 110, loss: 6.1305, perplexity: 459.67\n",
      "iter: 115, loss: 6.1312, perplexity: 459.97\n",
      "iter: 120, loss: 6.0925, perplexity: 442.51\n",
      "iter: 125, loss: 6.0848, perplexity: 439.11\n",
      "iter: 130, loss: 6.1410, perplexity: 464.52\n",
      "iter: 135, loss: 6.0934, perplexity: 442.94\n",
      "iter: 140, loss: 6.1050, perplexity: 448.10\n",
      "iter: 145, loss: 6.1664, perplexity: 476.47\n",
      "iter: 150, loss: 6.0887, perplexity: 440.84\n",
      "iter: 155, loss: 6.0179, perplexity: 410.72\n",
      "iter: 160, loss: 6.1485, perplexity: 468.04\n",
      "iter: 165, loss: 6.1401, perplexity: 464.11\n",
      "iter: 170, loss: 6.0122, perplexity: 408.40\n",
      "iter: 175, loss: 6.2203, perplexity: 502.87\n",
      "iter: 180, loss: 5.9957, perplexity: 401.69\n",
      "iter: 185, loss: 6.0381, perplexity: 419.09\n",
      "iter: 190, loss: 6.0215, perplexity: 412.21\n",
      "iter: 195, loss: 6.1388, perplexity: 463.48\n",
      "[',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',']\n",
      "[',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',']\n",
      "[',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',']\n",
      "[',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',']\n",
      "[',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',']\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=g) as sess:\n",
    "  writer = tf.summary.FileWriter(os.path.join(LOG_DIR), sess.graph)\n",
    "  batch_size = 64\n",
    "  fetches = {\n",
    "    \"loss\": model.loss,\n",
    "    \"perplexity\": model.perplexity,\n",
    "    \"train_op\": model.train_op\n",
    "  }\n",
    "  \n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  for i in range(200):\n",
    "    input_data, labels = get_batch(text_data_trn, 64, T)\n",
    "    feed_dict = {\n",
    "      model.input_data: input_data,\n",
    "      model.labels: labels\n",
    "    }\n",
    "    result = sess.run(fetches, feed_dict)\n",
    "    if i % 5 == 0:\n",
    "      print(\"iter: {}, loss: {:.4f}, perplexity: {:.2f}\".format(\n",
    "          i, result['loss'], result['perplexity']))\n",
    "  # make prediction\n",
    "  input_data, labels = get_batch(text_data_trn, 64, T)\n",
    "  feed_dict = {\n",
    "    model.input_data: input_data,\n",
    "    model.labels: labels\n",
    "  }\n",
    "  predictions = sess.run(model.predictions, feed_dict)\n",
    "  for prediction in predictions[:5]:\n",
    "    print(word_dict.ids_to_tokens(prediction))\n",
    "  \n",
    "  saver = tf.train.Saver()\n",
    "  saver.save(sess, os.path.join(LOG_DIR, 'language_model.ckpt'), global_step=global_step)\n",
    "  embed_meta.write_embed_meta(LOG_DIR, word_dict, model.embeddings, writer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
